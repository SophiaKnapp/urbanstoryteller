{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "from utils.utils import load_dataframes, load_dataframe, write_df_to_csv, write_dict_to_csv\n",
    "from utils.paths import HASHTAG_FREQUENCY_DIR, HASHTAGS_PER_USER_DIR, GREEDY_MODULARITY_DIR, make_dirs, make_dir\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from itertools import combinations\n",
    "\n",
    "make_dirs()\n",
    "\n",
    "min_count = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_communities(df_frequencies, df_users, min_count, forbidden_hashtags=[]):\n",
    "    df_frequencies.drop(df_frequencies[df_frequencies['count'] < min_count].index, inplace = True)\n",
    "    G = nx.Graph()\n",
    "    hashtags = [x for x in list(df_frequencies['hashtag']) if x not in forbidden_hashtags]\n",
    "    df_users['hashtags'] = df_users['hashtags'].apply(lambda x: [hashtag for hashtag in x if hashtag in hashtags and not hashtag in forbidden_hashtags])\n",
    "\n",
    "    df_frequencies = df_frequencies[~ df_frequencies['hashtag'].isin(forbidden_hashtags)]\n",
    "\n",
    "    # Add a node to the graph for every hashtag\n",
    "    for index, row in df_frequencies.iterrows():\n",
    "        G.add_node(row.hashtag, count=row['count'])\n",
    "    \n",
    "    # For every user: add edges in between all nodes corresponding to pairs or hashtags that they mentioned\n",
    "    for index, row in df_users.iterrows():\n",
    "        hashtags = row.hashtags\n",
    "        pairs = list(combinations(hashtags, 2))\n",
    "\n",
    "        # Add a 1/(number of hashtags that a user mentioned) to the weight of every edge that the user is involved in\n",
    "        if len(hashtags) > 0:\n",
    "            weight = 1/(len(hashtags)) \n",
    "            for pair in pairs: \n",
    "                if pair in G.edges:\n",
    "                    data = G.get_edge_data(pair[0], pair[1])\n",
    "                    G.add_edge(pair[0], pair[1], weight=data['weight']+weight)\n",
    "                else:\n",
    "                    G.add_edge(pair[0], pair[1], weight=weight)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clusters(out_super_dir, min_count, quarter, resolution, df_frequency, df_user, forbidden_hashtags):\n",
    "    out_dir = os.path.join(out_super_dir, quarter)\n",
    "    make_dir(out_dir)   \n",
    "    G = generate_graph_communities(df_frequency, df_user, min_count, forbidden_hashtags)\n",
    "\n",
    "\n",
    "    if len(G.edges) > 0:\n",
    "        communities = greedy_modularity_communities(G, weight='weight', resolution=resolution)\n",
    "\n",
    "        all_clusters = {}\n",
    "\n",
    "        for community in communities:\n",
    "            H = G.subgraph(community)\n",
    "            weights = nx.get_edge_attributes(H, 'weight')\n",
    "            counts = nx.get_node_attributes(H,'count')\n",
    "            community_size = sum(weights.values())\n",
    "            hashtags = dict(sorted(counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "            list_most_common = list(hashtags.keys())\n",
    "\n",
    "            length_cluster_name = min(len(list_most_common), 3)\n",
    "            most_common = ''\n",
    "            for i in range(length_cluster_name):\n",
    "                most_common = most_common + list_most_common[i] + '_'\n",
    "            all_clusters[most_common] = community_size\n",
    "            write_dict_to_csv(hashtags, most_common, out_dir, ['hashtag', 'count'])\n",
    "\n",
    "        all_clusters = dict(sorted(all_clusters.items(), key=lambda item: item[1], reverse=True))\n",
    "        write_dict_to_csv(all_clusters, 'all_clusters', out_dir, ['cluster', 'n_users'])\n",
    "    \n",
    "    \n",
    "def make_greedy_modularity_sub_dir(min_count, resolution):\n",
    "    out_dir = os.path.join(GREEDY_MODULARITY_DIR, 'min_count_' + str(min_count) + '_res_' + str(resolution))\n",
    "    make_dir(out_dir)\n",
    "    return out_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find clusters of related hashtags\n",
    "- Generate a graph from the hashtags used in every districts\n",
    "- Find communities of hashtags that are frequently mentioned together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_per_user_dict = load_dataframes(HASHTAGS_PER_USER_DIR, 2)\n",
    "frequencies = load_dataframes(HASHTAG_FREQUENCY_DIR)\n",
    "\n",
    "min_count = 10\n",
    "resolution = 1.8\n",
    "\n",
    "out_dir = make_greedy_modularity_sub_dir(min_count, resolution)\n",
    "\n",
    "for quarter in hashtags_per_user_dict:\n",
    "    forbidden_hashtags = [quarter]\n",
    "    df_frequency = frequencies[quarter]\n",
    "    df_user = hashtags_per_user_dict[quarter]\n",
    "    find_clusters(out_dir, min_count, quarter, resolution, df_frequency, df_user, forbidden_hashtags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
